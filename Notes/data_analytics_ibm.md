
"Businesses today recognize the untapped value in data and data analytics as a crucial factor for business competitiveness. To drive their data and analytics initiatives, companies are hiring and Upskilling people. They are expanding their teams and creating centers of excellence to set up a multi-pronged data and analytics practice in their organizations." 
- The Power of Data to Transform Your Business (a Forrester Report)

Forbes 2020 Report:
"The constant increase in data processing speeds and bandwidth, the nonstop invention of new tools for creating, sharing and consuming data, and the steady addition of new data creators and consumers around the world, ensure that data growth continues unabated. Data begets more data in a constant virtuous cycle."
#Report Conclusion# 	data -> data -> data. Data cycle


# Data holds the key to competetive advantage.

* Data Engineer -> 
	- build and maintain data architectures, make data available for others
	- accesible data for APP's.
	
* Data Analyst:
	- translates data and numbers into plain language
	- inspects/cleans data
	- identifies PATTERNS, mines data
	- VISUALIZES data
	# data translating, inspecting, cleaning, identifying, mining, visualizing
	

* Data Scientists:
	- analyze data for actionable insights
	- create predictive models for ML / DL 
	- PREDICTIONS for customers
	- math, statistics, domain knowledge, programming
	# transform data into predictions and models 
	
* BI Analysts:
	- leverage the work of Data Analysts and Scientists 
		- to look at possible implications for their business and 
		  the actions they need to take or recommend
		
		
* Data Engineering: 
	converts raw data into usable data
* Data Analysts: 
	use this data to generate insights
* Data Scients: 
	use Data Analytics and Data Engineering to 
		PREDICT the future using data from the past, 
		make MODELS
* Business Analysts and Business Intelligience (BI) Analysts: 
	use these insights and predictions to drive decisions that 
		BENEFIT and grow their BUSINESS.
	
	
	
	
DATA ANALYSIS:
	gather, clean, analyze, mine data
	interpret results
	report the findings
	find patterns/correlations within data to generate insights, conclusions
	
	understand past perf
	take informed decisions
	validate course of action b4 commiting
	
	
4 TYPES:
	Descriptive Analytics: "what happened"
		provides insights to past events
	Diagnostic Analytics: "why did it happen"
		takes insights from g<1> to dig deeper
	Predictive Analysis: "what will happen next"
		leverages historical data and trends to predict future outcomes
	Prescriptive Analytics: "what should be done about it"
		analyzes past events to estimate the likelihood of different outcomes
		
	PROCESS:
		1- understanding the problem, desired outcome
		2- setting a clear metric, what and how should it be measured.
		3- gathering data, identify data you req, best tools, best sources for data
		4- cleaning data: fixing quality issues in data, standardize data coming from multiple sources.
		5- Analyzing and Mining data:
			extract, analyze, manipulate data from different perspectives to understand trends, find patterns
		6- Interpreting results:
			evaluating defendability of analysis 
		7- Presenting your findings:
			communicate, present.
			
		understand problem, wanted outcome.
		what and how, 
		gather, identify data, best tools? best sources?
		clean - standardization, fixes
		analyze, mine, extract, manipulate data - understand trends, find patterns
		interpret, present
			
WHAT IS DATA ANALYTICS:

- Collecting, analyzing data/info
- Confirm Hypo
- Storytelling with Data

- Use of info to make decisions

- Define the problem, create hypothesis, collect/clean data to test, analyze data, present.

- analyze sets of data to understand what's going on.

- understand where a business is coming from, it's present and predict its future decisions

- Analyze, present, share data. To communicate business insights, help make better decisions.

- Process of gathering info from relevant population. Breaking that info into subsets (cluster probability), using that data to make decisions, predictions.

# State differences between data analytics and data analysis, in the POV of statistics.
Data Analytics vs. Data Analysis:
	they are often used interchangeably.
	however there is a subtle diff.
	
	Analysis - detailed exam of the elements or structure of sth
	Analytics - the systematic computational analysis of data or statistics
		* for data and statistics POV, they are about same thing.
		
		
RESPONSIBILITIES:
	acquiring data
	creating quaries to extract data
	filtering, cleaning, standarzing, reorganizing data
	using statistical techniques (visualization, ggplot2, R)
	analyze patterns, trends
	prepare reports
	create approp. docs
	

- natural curiosity
- attention to detailed, looking for patterns

- Py, SQL, R, Tableau, Power BI

+ Sales pipeline analysis
+ financial reporting
+ headcount planning
+ all verticals, such as Airlines, Pharmas, Banks.


- acquire, prepare, analyze data. Interpret, communicate, doc.

Data: 
*Structured (databases, datasets) # what is structured data? Give examples
	-> tabular mannor -row,columns
	-> SQL databases
	-> OTP (online transcation processing)
	-> GPS, RFID tags
	-> network-web server logs
	-> online forms/spreadsheets
*Semi-structure (emails),
	- tags, elements (metadata)
	- groups data to hierarchy
	* e-mails
	* binary exe's
	* .zip
	* XMLs, JSONs
*Unstructured (photos, pdfs, social media).
	- .html, .img, .mp4, .mp3, .pdf, .doc, .ppt
	- social media, 
	- NoSQL
	
Data Repo's:
	databases, data warehouses, data lakes, data marts, BigData stores/warehouses.
	

Query Lang: SQL
Programming Lang: Py
Shell: Bash

gather,extract,transform,load
data wrangling,clean
data analysis, mining
data visualization


Different Types of File Formats:

.csv: delimited text file formats
	-> used to store data as text
	.tsv : tab-seperated val's

.xlsx: Microsoft Excel Open .XML Spreadsheet
		- a spreadsheet
		- rows, columns, cells
		- Structured.
		- secure
.xml : extensible markup language
		- readable by humans and machines
		- similar to .html
		- self-descriptive language
		- platform independent
		- prog lang independent
.pdf : Portable Document Format
		- viewed same on any device
		- by Adobe
.json : JavaScript Object Notation
		- prog lang independent
		- easy to use
		- one of the best tools for sharing data
		- used to return data by many services

SOURCES of DATA :=

* Relational Databases:
	- business activities
	- M\* SQL Server
	- Oracle
	- MySQL
	- IBM DB2
	- can be used for analysis, predictions
	
* Flat Files:
- store data in plain text
- one recor per line/row
- maps to a single table
- like .csv

* Spreadsheet files:
- special type of flat file
- organizes data in a tabular format
- .XLS, .XLSV
- Google Sheets, Apple Numbers, LibreOffice

** XML Files:
- can support complex data structures (i.e. hierarchal)
- online surveys, bank statements etc. (unstructured data sets)

*** API and Web Services:
- Application Program Interfaces (API)
- can return data in plain text, such as XML, HTML, JSON.

- req.web() [from users] 
- req.network() [API].

- TW, FB APIs
- Stock Market APIs
- Data Lookup and Valid APIs

** Web Scraping
(a.k.a -- screen scraping, web harvesting, web data extraction)
- extract data from unstructured sources
- downloads specific data

* apps like IDM do web scraping
* BeautifulSoup, Scrapy, Pandas, Selenium


Data Streams and Feeds:
- Aggregating streams of data
- flowing from,
	IoT devices and apps,
	GPS data from cars,
	PC programs,
	websites.
	
* stock tickers for trading
* video feeds for threat detection
* web click feeds for monitoring
* real-time flight events
** APACHE kafka, spark, storm

RSS (Really Simple Syndication) Feeds:
- capturing updated data from forums and news sites.
- data is constantly refreshed.


LANGUAGES for DATA PROFESSIONALS:

* Query L: 
	- accessing and manipulating data in a database
	- SQL
* Programming L:
	- Python, R, Java
	- developing apps
* Shell L:
	-ideal for repetitive operational tasks
	- Bash, PowerShell
	
SQL : Structured Query Language
	designed for accessing and manipulating info from mainly relational databases
	
	- Insert, update, delete records in a database
	- Create new databases, tables, views
	- Write stored procedures.
	(call them for later use)
	
	* Portable, platform independent
	* can be used for querying data in a wide variety of databases
	* simple syntax
	* fewer lines of code
	* interpreter system
	* retrieve large data quick and efficient
	
Py:
	pandas : data cleaning, analysis
	numpy, scipy : statistics
	beautifulsoup, scrapy : web scraping
	Matplotlib, Seaborn : visualization
	Opency : image processing
	

* Overview of Data Repo's:
- databases, data warehouses, Big Data stores
	Database:
		collection of data for input, storage, search, retrieval and modification of data
	DBMS: (Database Management System) 
		a set of programs for creating and maintaining the db, and storing, modifying and extracting info from the db.
		* uses "Querying" function.
		
		- Factors like,
		Data type, 
		data structure, 
		querying mechanisms, 
		latency req's, 
		transaction speeds, 
		intended use
			- governs the choice of database.
			
	RDBMS: (Relational DBs)
	- data is organized
	- tabular format, rows & columns
	- well-defined structure
	- optimized for data operations and querying
	* Standard tool: SQL 
		
	NoSQL: (Non-Relational Databases)
	* "Not Only SQL"
	- emerged in response,
		to the Volume, speed and diversity of data generation
			i.e. Cloud Computing, IoT, social media.
	- build for speed, flex, scale:
		schema-less form of data.
	- Big Data Processing.
	
Data Warehouse:
	- works as a central repo
		that merges info coming from a disperate sources and consolidates it through the extract, transform and load (ETL) process. 
			into one comprehensive db for analytics and BI.
	- Although historically it was only using RDBMS, due to technical breakthroughs, both NoSQL and RDBMS (SQL) repo's are now used in Data Warehouses.
	
	Extract, Transform, Load (ETL) Process:
		- Extract Data 
			from different sources
		- transform data
			to clean, usable state.
		- Load Data
			into the data repo.
	
Big Data Stores:
	* Distributed computational and storage infrastructure to store, scale and process very large datasets.

SUMMARY:
* Data repo's help to isolate data,
	* and make reporting and analytics more efficient and credible
		* while also serving as a data archive.


RDBMS:
	rows -> records
	columns - > attributes

	- based on flat files i.e. spreadsheets
	- ideal for optimized storage, retrieval, processing for LARGE volumes of data
	- relationships between tables
	- restrictions can be made
	* SQL / can retrieve millions of records in seconds
	
	* IBM DB2, Microsoft SQL Server, MySQL, Oracle Database, PostgreSQL
	* Amazon RDS, Google Cloud SQL, IBM DB2 Cloud, Oracle Cloud, SQL Azure 
	
	- creating meaningful info
	- flexibility
	- minimize data redundancy 
	- easy backup/recovery
	- ACID complient!
	
	* ACID: 
		Atomicity
		Consistency
		Isolation
		Durability
		
* Suitable For:
	- OLTP application 
	(Online Transaction Processing)
	- Data Warehouses
	-IoT solutions
	
*Limitations
	- works well for structured data only
	- merging is difficult
	- greater val than defined gets lost
	
NoSQL DB:
	not only SQL 
	non SQL
	
	- scale, performance, ease of use
	- built for specific data models
	- not traditional row/column/table designed
	- schema-less / free-form storing
	
	* Key-val store:
		key:attribute
		no relationships
		no multiple keys
		
		- Redis, Memcached, DynamoDB
	* Documnet-based:
		+ flexible indexing
		+ powerful ad hoc queries
		+ analytics over docs
		- complex queries
		- multi-op transactions
		: MongoDB, CouchDB, Cloudant, DocumentDB
	* Column-based:
		+ column family (groping of columns)
		+ easier, faster accessing/searches
		+ great for heavy write/read requests 
		- running complex queries
		- change patterns frequently
		: cassandra, hbase
		
	* Graph-based
		+ visualizing, interpreting, relationdships
		+ social networks, network diagrams, product recommendations, access management (face unlock etc.)
		+ neo4j, cosmosDB
		
+ ability to handle LARGE VOLS of data
+ distributed system
+ effective, efficient


RDBMS : NOSQL
typed/composed : schema-less
expensive : low-cost
ACID-compliance : not 
mature, easy troubleshoot : new, can be tricky


* Data Marts, Data Lakes, ETL, Data Pipelines:
	- Data Warehouse:
		multi-purpose storage
		data has already gone through ETL process, ready to analysis/mining
	
	- Data Marts:
		a specific DWH.
		built for some purpose
		* giving stakeholders the relevant data, when they need it.
		restricted, isolation, security
	
	- Data Lakes:
		storage repo
		any RAW data
		classified, tagged as metadata
		* "STAGING AREA" of a DWH.
		predictive, advanced analysis
	
	
	- ETL (Extract, Transform, Load) Process:
		gather raw data
		extract relevant info
		clean, standardize, transform
		load into a data-repo
		
	- Extract: 
		raw data -> staging area
		
		Batch Processing:
			move large data, at scheduled intervals
			* Blendo, Stitch
		Stream Processing:
			pull real-time data, transform mid-transit
			* Apache Samza, Storm and Kafka
			
	- Transform
		standardize (km, dd/mm/yy...)
		remove NA, duplicates
		filter()
		enrich data (i.e. name A B = A, B)
		key-relationships
		data validations, rules
		
		* Initial Loading:
			all data into repo
		* Incremental Loading:
			periodically update/modify
		* Full-refresh:
			erase, reload data
		
* Data Pipeline:
	- the whole journey of moving data
	- includes ETL
	- batch, stream data
	* Apache Beam and Kafka, DataFlow
	

Foundations of Big Data:
	- Ernst and Young:
		* big data refers to the dynamic, large, and disparate volumes of data being created by people, tools, and machines. It requires new, innovative and scalable technology to collect, host, and analytically process the vast amount of data gathered in order to drive real-time business insights that relate to consumers, risk, profit, performance, productivity management, and enhanced shareholder value.
		
		- 5V's:
		Velocity : extremely fast, never stops
		Volume : scale
		Variety : SQL, NoSQL, schema-less
		Veracity : quality, origin, consistency, completeness, integrity, ambiguity
		Value : profit, benefits, satisfaction

* Apache Hadoop, Hive, Spark.
	- Hadoop: 
		distributed storage and processing of BD
		Java-based
		Node : a computer
		Cluster : >1 computer
		
		reliable, scalable, cost-effective
		
	- Hive:
		DWH
		- high latency
		+ ETL, data analysis
		+ SQL
		
	- Spark:
		distributed data analytics framework 
		Analysis, Processing, ML, Data Integration, ETL
		
* OLTP: Online Transaction Processing
	

### Week 3 
### Identfying Data for Analysis

- what info
	your goal?
- which source
- plan
	time frame
	volume, amount
	define dependencies and risks
- methods
- key considerations
	quality, security, privacy
- for reliable data
	no errors
	accurate
	complete
	relevant
	accessible
- data governance
	sevurity, regulation, compliances
- data privacy
	confidentiality, licences, compliance to regulations

* right data =>
	allow looking at multiple perspectives
	credible&reliable findings

# Data Sources
* Internal 
- External
* Primary
	obtained directly from source
	internal (CRM, HR, worklow applications)
	external (surveys, interviews, discussions, observations, focus groups)
- Secondary 
	from existing sources
	external db's
	research articles, publications, internet (public)
	data collected from public external 
- Third-party
	purchased from aggregators who collect data

* Sources
	* Dynamic
	* Diverse
	* Continuously evolving

	- Databases
		- Cloud
	- Web
		- social media
	- sensor data
		- from smart apps, wearable devices
	- data exchange
		3rd party
		voluntary sharing of data 
	- surveys
	- Census
		household data as wealth, income, population
	- Interviews
		qualitative data
	- Observation Studies


# How to Gather & Import Data

- using queries (SQL)
	to extract data from SQL DB's (RDBMS)
- NRDBMS can be queried using SQL(-like) query tools
	or specific querying tools i.e. 
		CQL for Cassandra
		GraphQL for Neo4J

- API's
	invoked from apps
	can be used for data validation

- web scraping (screen scraping, web harvesting)
	download things like
		text, info, images, videos, music etc.

- RSS feeds (ongoing refreshed data)
	i.e. forums

- sensor data
	IoT devices, apps, GPS

- Data Exchange platforms
	- well-defined protocols
	- security, governance
	* AWS DataExchange
	* Crunchbase
	* Lotame
	* Snowflake


* Other Sources
	- research&advisory firms; i.e.
	+ Forrester, Gartner, Business Insider
	- agencies: market surveys, demographic studies

### Importing Data

# Identified and Gathered Data =>
	Data Repo

# Wrangling Data (data munging)
	data exploration 
		understand your data better @ use case
	transformation
		structuring
			change form & schema
				to i.e. combining RDBMS&Web APIs' data
				joins	
					combines columns 
				unions
					combines rows
		normalizing
			cleaning 
			improving 
			reduce redundancy & inconsistency
		denormalizing
			combines data 
			for faster querying
		cleaning (filtering)
			inaccuracies
			NAs
			nulls
			incomplete
			biases
			outliers
		enriching
			adding stuff to make it more meaningful
			points,
			relationships,
			visualizations
	validation
		check consistency, quality, security
	publishing
	documentation

# Tools for Data Wrangling
	Excel Power Quary / Spreadsheets
	OpenRefine
		open-source
		can import&export data
			json, tsv, csv, xls, xml (transformation)
	Google DataPrep
		fully-managed service
		@ cloud
		smart
	Watson Studio Refinery
		IBM Watson Studio
		smart
		has built-in features to discover, cleanse, transform d
	Trifacta Wrangler
		export to excel, tableau, R
		collab features
	Python
		Jupyter Notebook
		NumPy
		SciPy
		pandas
			data analysis
			merging, joining, transforming 
	R
		Dplyr
		Data.table
			aggregate large data sets quickly
		jsonlite
			json parsing tool, @ web API interactions
	

# DATA CLEANING
1) Inspection
	detect issues, errors
	data profiling
	data visualization
2) Cleaning
	@ use case
	NA's -> bias & wrong results
	duplicates
	irrelevant data (filtering)
	make sure formats are same (looking at you, yy/dd/mm haha)
	outliers, bad skew
3) Verification 
	> run summary statistics
		to makes ure it is consistent w/ reality
	